{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "203e8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8967f381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Carl Whitebeck died in July 2021 Carl Whitebec...</td>\n",
       "      <td>Carl Whitbeck’s sudden death in July 2021 leav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carl Whitbeck took over management in June 201...</td>\n",
       "      <td>Carl \"Pepper\" Whitbeck formally took over the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Carl Whitbeck has been a member of the team si...</td>\n",
       "      <td>Whitbeck had been a member of the team since 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>Michael Graham worked alongside Carl Whitbeck ...</td>\n",
       "      <td>Michael Graham, who worked alongside Whitbeck ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>Robert Houle is the US high-yield portfolio ma...</td>\n",
       "      <td>Robert Houle, US high-yield portfolio manager ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              input  \\\n",
       "0           0  Carl Whitebeck died in July 2021 Carl Whitebec...   \n",
       "1           3  Carl Whitbeck took over management in June 201...   \n",
       "2           5  Carl Whitbeck has been a member of the team si...   \n",
       "3           8  Michael Graham worked alongside Carl Whitbeck ...   \n",
       "4          13  Robert Houle is the US high-yield portfolio ma...   \n",
       "\n",
       "                                              output  \n",
       "0  Carl Whitbeck’s sudden death in July 2021 leav...  \n",
       "1  Carl \"Pepper\" Whitbeck formally took over the ...  \n",
       "2  Whitbeck had been a member of the team since 2...  \n",
       "3  Michael Graham, who worked alongside Whitbeck ...  \n",
       "4  Robert Houle, US high-yield portfolio manager ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset_merged_notes.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5a480c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carl Whitebeck died in July 2021 Carl Whitebeck’s death leaves a significant gap for the team The People Pillar was downgraded from Above Average to Average'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given input text\n",
    "dataset['input'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3341feb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carl Whitbeck’s sudden death in July 2021 leaves a significant gap in overall experience for the team and leads to a downgrade of the People Pillar to Average from Above Average. '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target text\n",
    "dataset['output'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ae7238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['max_length_input'] = dataset['input'].apply(lambda x:len(x.split(' ')))\n",
    "dataset['max_length_input'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e307b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['max_length_output'] = dataset['output'].apply(lambda x:len(x.split(' ')))\n",
    "dataset['max_length_output'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b15fac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(803,) (90,) (803,) (90,)\n"
     ]
    }
   ],
   "source": [
    "X = dataset['input']\n",
    "y = dataset['output']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10,random_state = 42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54f8fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train,y_train],axis=1, ignore_index=True)\n",
    "test_df = pd.concat([X_test, y_test],axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec2fa45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df= train_df.rename(columns={0:'input',1:'output'})\n",
    "test_df= test_df.rename(columns={0:'input',1:'output'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f95daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   dev = torch.device(\"cuda:0\")\n",
    "   print(\"Running on the GPU\")\n",
    "else:\n",
    "   dev = torch.device(\"cpu\")\n",
    "   print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f81d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "num_of_batches=int(len(X_train)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aba238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "def progress(loss,value, max=100):\n",
    " return HTML(\"\"\" Batch loss :{loss}\n",
    "      <progress    \n",
    "value='{value}'max='{max}',style='width: 100%'>{value}\n",
    "      </progress>\n",
    "             \"\"\".format(loss=loss,value=value, max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "376f33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"JulesBelveze/t5-small-headline-generator\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"JulesBelveze/t5-small-headline-generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f6de5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6357674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 11:36:21.629321: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 11:36:21.781249: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-15 11:36:21.813154: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import Adafactor\n",
    "optimizer = Adafactor(model.parameters(),lr=1e-3,\n",
    "                      eps=(1e-30, 1e-3),\n",
    "                      clip_threshold=1.0,\n",
    "                      decay_rate=-0.8,\n",
    "                      beta1=None,\n",
    "                      weight_decay=0.0,\n",
    "                      relative_step=False,\n",
    "                      scale_parameter=False,\n",
    "                      warmup_init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc468796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainfun(model,train_df,batch_size,num_of_epochs,optimizer):\n",
    "  model.train()\n",
    "  num_of_batches=int(len(train_df)/batch_size)\n",
    "  loss_per_10_steps=[]\n",
    "  for epoch in range(1,num_of_epochs+1):\n",
    "    print('Running epoch: {}'.format(epoch))\n",
    "    running_loss=0\n",
    "    out = display(progress(1, num_of_batches+1), display_id=True)\n",
    "    for i in range(num_of_batches):\n",
    "      # print(f'Batch {i}')\n",
    "      inputbatch=[]\n",
    "      labelbatch=[]\n",
    "      new_df=train_df[i*batch_size:i*batch_size+batch_size]\n",
    "      for indx,row in new_df.iterrows():\n",
    "        input = str(row['input']) + '[SEP]' \n",
    "        labels = row['output']+'[SEP]'   \n",
    "        inputbatch.append(input)\n",
    "        labelbatch.append(labels)\n",
    "      inputbatch=tokenizer.batch_encode_plus(inputbatch,padding=True,max_length=512,return_tensors='pt')[\"input_ids\"]\n",
    "      labelbatch=tokenizer.batch_encode_plus(labelbatch,padding=True,max_length=512,return_tensors=\"pt\") [\"input_ids\"]\n",
    "      inputbatch=inputbatch.to(dev)\n",
    "      labelbatch=labelbatch.to(dev)\n",
    "\n",
    "    # clear out the gradients of all Variables \n",
    "      optimizer.zero_grad()\n",
    "\n",
    "    # Forward propogation\n",
    "      outputs = model(input_ids=inputbatch, labels=labelbatch)\n",
    "      loss = outputs.loss\n",
    "      loss_num=loss.item()\n",
    "      logits = outputs.logits\n",
    "      running_loss+=loss_num\n",
    "      if i%10 ==0:      \n",
    "        loss_per_10_steps.append(loss_num)\n",
    "      out.update(progress(loss_num,i, num_of_batches+1))\n",
    "\n",
    "    # calculating the gradients\n",
    "      loss.backward()\n",
    "\n",
    "    #updating the params\n",
    "      optimizer.step()\n",
    "    \n",
    "    running_loss=running_loss/int(num_of_batches)\n",
    "    print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n",
    "  return model,loss_per_10_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a1ea6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.5584446787834167\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2288: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Running loss: 1.3118797218799592\n",
      "Running epoch: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.46791520714759827\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 , Running loss: 0.936604059934616\n",
      "Running epoch: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.38803720474243164\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 , Running loss: 0.7693475010991097\n",
      "Running epoch: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.29087182879447937\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 , Running loss: 0.6386855405569076\n",
      "Running epoch: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.27510765194892883\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 , Running loss: 0.5482774794101715\n",
      "Running epoch: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.21496571600437164\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 , Running loss: 0.4483312977850437\n",
      "Running epoch: 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.18729786574840546\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 , Running loss: 0.3845948837697506\n",
      "Running epoch: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.15950173139572144\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 , Running loss: 0.3208825723826885\n",
      "Running epoch: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.15498383343219757\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 , Running loss: 0.2683559723198414\n",
      "Running epoch: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.12159062922000885\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 , Running loss: 0.22361986547708512\n",
      "Running epoch: 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.10857673734426498\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 , Running loss: 0.18686548475176096\n",
      "Running epoch: 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.10220381617546082\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 , Running loss: 0.1673224437981844\n",
      "Running epoch: 13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.07075969874858856\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 , Running loss: 0.14868485525250436\n",
      "Running epoch: 14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.06623754650354385\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 , Running loss: 0.1278131880238652\n",
      "Running epoch: 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Batch loss :0.0765661969780922\n",
       "      <progress    \n",
       "value='99'max='101',style='width: 100%'>99\n",
       "      </progress>\n",
       "             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 , Running loss: 0.1067809484526515\n"
     ]
    }
   ],
   "source": [
    "model,loss_per_10_steps=trainfun(model,train_df,batch_size,15,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c886d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"t5_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6af943be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, sent):\n",
    "  model.eval()\n",
    "  sent1 = str(sent) + \"[SEP]\"\n",
    "  input_ids = tokenizer.encode(sent1, return_tensors=\"pt\",max_length=512,padding=True)\n",
    "  input_ids=input_ids.to(dev)\n",
    "  outputs = model.generate(input_ids,num_beams=8, do_sample=True, min_length=10, max_length=512)\n",
    "  #print(outputs)\n",
    "  z=tokenizer.decode(outputs[0],skip_special_tokens=True,min_length=512)\n",
    "  return(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predicted']=test_df['input'].apply(lambda x: prediction(model,x))\n",
    "test_df['predicted']=test_df['predicted'].apply(lambda x: x.replace('[SEP]',''))\n",
    "test_df.to_csv('res_testset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ramsrigouthamg/t5-large-paraphraser-diverse-high-quality\")\n",
    "\n",
    "\n",
    "def paraphrase(s):   \n",
    "    context = s\n",
    "    text = \"paraphrase: \"+context + \" </s>\"\n",
    "\n",
    "    encoding = tokenizer.encode_plus(text,max_length =128, padding=True, return_tensors=\"pt\")\n",
    "    input_ids,attention_mask  = encoding[\"input_ids\"].to(dev), encoding[\"attention_mask\"].to(dev)\n",
    "    model.eval()\n",
    "    diverse_beam_outputs = model.generate(\n",
    "        input_ids=input_ids,attention_mask=attention_mask,\n",
    "        max_length=128,\n",
    "        early_stopping=True,\n",
    "        num_beams=5,\n",
    "        num_beam_groups = 5,\n",
    "        num_return_sequences=1,\n",
    "        diversity_penalty = 0.70\n",
    "    )\n",
    "#     print (\"Original: \",context)\n",
    "    for beam_output in diverse_beam_outputs:\n",
    "        sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "        return (sent)\n",
    "    \n",
    "test_df['paraphrased'] = test_df['predicted'].apply(paraphrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ef08d02c71bdd08f1b408eeef22b0653d00b319fed3388cd0e2d80c9a94a1c2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
